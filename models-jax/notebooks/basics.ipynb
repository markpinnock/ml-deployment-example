{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        1.05      2.1       3.1499999 4.2      ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Jax numpy\"\"\"\n",
    "\n",
    "\n",
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "\n",
    "x = jnp.arange(5.0)\n",
    "print(selu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Random number generation\"\"\"\n",
    "\n",
    "key = jax.random.key(1701)\n",
    "x = jax.random.normal(key, (1_000_000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173 μs ± 5.62 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "23 μs ± 1.67 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"JIT compilation\"\"\"\n",
    "%timeit selu(x).block_until_ready()\n",
    "\n",
    "selu_jit = jax.jit(selu)\n",
    "_ = selu_jit(x)\n",
    "\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661194 0.10499357]\n",
      "[0.24998187 0.1965761  0.10502338]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Gradients\"\"\"\n",
    "\n",
    "\n",
    "def sum_logistic(x):\n",
    "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "\n",
    "x_small = jnp.arange(3.0)\n",
    "deriv_fn = jax.grad(sum_logistic)\n",
    "print(deriv_fn(x_small))\n",
    "\n",
    "\n",
    "def finite_diff(f, x, eps=1e-3):\n",
    "    return jnp.array(\n",
    "        [(f(x + eps * v) - f(x - eps * v)) / (2 * eps) for v in jnp.eye(len(x))]\n",
    "    )\n",
    "\n",
    "\n",
    "print(finite_diff(sum_logistic, x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.        0.       ]\n",
      " [0.        2.7182817 0.       ]\n",
      " [0.        0.        7.389056 ]]\n",
      "[[-0.         -0.         -0.        ]\n",
      " [-0.         -0.09085774 -0.        ]\n",
      " [-0.         -0.         -0.07996248]]\n",
      "[[-0.         -0.         -0.        ]\n",
      " [-0.         -0.09085774 -0.        ]\n",
      " [-0.         -0.         -0.07996248]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Jacobians and Hessians\"\"\"\n",
    "\n",
    "print(jax.jacobian(jnp.exp)(x_small))\n",
    "\n",
    "\n",
    "def hessian(fn):\n",
    "    return jax.jit(jax.jacfwd(jax.jacrev(fn)))\n",
    "\n",
    "\n",
    "print(hessian(sum_logistic)(x_small))\n",
    "print(jax.hessian(sum_logistic)(x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naively batched\n",
      "729 μs ± 31.6 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "Manually batched\n",
      "24.4 μs ± 1.92 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "Auto-vectorized with vmap\n",
      "23 μs ± 1.93 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"jax.vmap()\"\"\"\n",
    "\n",
    "key1, key2 = jax.random.split(key)\n",
    "mat = jax.random.normal(key1, (150, 100))\n",
    "batched_x = jax.random.normal(key2, (10, 100))\n",
    "\n",
    "\n",
    "def apply_matrix(x):\n",
    "    return jnp.dot(mat, x)\n",
    "\n",
    "\n",
    "def naively_batched_apply_matrix(batched_x):\n",
    "    return jnp.stack([apply_matrix(x) for x in batched_x])\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def batched_apply_matrix(batched_x):\n",
    "    return jnp.dot(batched_x, mat.T)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def vmap_batched_apply_matrix(batched_x):\n",
    "    return jax.vmap(apply_matrix)(batched_x)\n",
    "\n",
    "\n",
    "print(\"Naively batched\")\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()\n",
    "print(\"Manually batched\")\n",
    "_ = batched_apply_matrix(batched_x).block_until_ready()\n",
    "%timeit batched_apply_matrix(batched_x).block_until_ready()\n",
    "print(\"Auto-vectorized with vmap\")\n",
    "_ = vmap_batched_apply_matrix(batched_x).block_until_ready()\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()\n",
    "\n",
    "np.testing.assert_allclose(\n",
    "    naively_batched_apply_matrix(batched_x),\n",
    "    batched_apply_matrix(batched_x),\n",
    "    atol=1e-4,\n",
    "    rtol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[3]. let\n",
      "    b:bool[3] = gt a 0.0\n",
      "    c:f32[3] = exp a\n",
      "    d:f32[3] = mul 1.6699999570846558 c\n",
      "    e:f32[3] = sub d 1.6699999570846558\n",
      "    f:f32[3] = pjit[\n",
      "      name=_where\n",
      "      jaxpr={ lambda ; g:bool[3] h:f32[3] i:f32[3]. let\n",
      "          j:f32[3] = select_n g i h\n",
      "        in (j,) }\n",
      "    ] b a e\n",
      "    k:f32[3] = mul 1.0499999523162842 f\n",
      "  in (k,) }\n",
      "{ lambda ; a:f32[3]. let\n",
      "    b:f32[3] = neg a\n",
      "    c:f32[3] = exp b\n",
      "    d:f32[3] = add 1.0 c\n",
      "    e:f32[3] = div 1.0 d\n",
      "    f:f32[] = reduce_sum[axes=(0,)] e\n",
      "  in (f,) }\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Jax expression\"\"\"\n",
    "\n",
    "print(jax.make_jaxpr(selu)(x_small))\n",
    "print(jax.make_jaxpr(sum_logistic)(x_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array((), dtype=key<fry>) overlaying:\n",
      "[ 0 43]\n",
      "0.07520543\n",
      "0.07520543\n",
      "draw 0: -1.9133632183074951\n",
      "draw 1: -1.4749839305877686\n",
      "draw 2: -0.36703771352767944\n",
      "Array((), dtype=key<fry>) overlaying:\n",
      "[3722464693 2600049559] [Array((), dtype=key<fry>) overlaying:\n",
      "[1615207904  772808876], Array((), dtype=key<fry>) overlaying:\n",
      "[ 153309274 3877468463], Array((), dtype=key<fry>) overlaying:\n",
      "[911978728  92600883]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"PRNG keys\"\"\"\n",
    "\n",
    "key = jax.random.key(43)\n",
    "print(key)\n",
    "\n",
    "print(jax.random.normal(key))\n",
    "print(jax.random.normal(key))\n",
    "\n",
    "for i in range(3):\n",
    "    new_key, sub_key = jax.random.split(key)\n",
    "    del key\n",
    "    val = jax.random.normal(sub_key)\n",
    "    del sub_key\n",
    "    print(f\"draw {i}: {val}\")\n",
    "    key = new_key\n",
    "\n",
    "key, *subkeys = jax.random.split(key, num=4)\n",
    "print(key, subkeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07592554 0.60576403 0.4323065 ]\n",
      "[-0.02830462  0.46713185  0.29570296]\n",
      "[0.07592554 0.60576403 0.4323065 ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Lack of sequential equivalence\"\"\"\n",
    "\n",
    "key = jax.random.key(42)\n",
    "subkeys = jax.random.split(key, num=3)\n",
    "sequences = np.stack([jax.random.normal(subkey) for subkey in subkeys])\n",
    "print(sequences)\n",
    "\n",
    "key = jax.random.key(42)\n",
    "print(jax.random.normal(key, shape=(3,)))\n",
    "\n",
    "key = jax.random.key(42)\n",
    "subkeys = jax.random.split(key, num=3)\n",
    "print(jax.vmap(jax.random.normal)(subkeys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call 1\n",
      "Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>\n",
      "1.5849625\n",
      "Call 2\n",
      "1.5849625\n",
      "{ lambda ; a:f32[]. let\n",
      "    b:f32[] = pjit[\n",
      "      name=log2_with_print\n",
      "      jaxpr={ lambda ; c:f32[]. let\n",
      "          d:f32[] = log c\n",
      "          e:f32[] = log 2.0\n",
      "          f:f32[] = div d e\n",
      "        in (f,) }\n",
      "    ] a\n",
      "  in (b,) }\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Impure functions\"\"\"\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def log2_with_print(x):\n",
    "    print(x)\n",
    "    ln_x = jnp.log(x)\n",
    "    ln_2 = jnp.log(2.0)\n",
    "    return ln_x / ln_2\n",
    "\n",
    "\n",
    "print(\"Call 1\")\n",
    "print(log2_with_print(3.0))\n",
    "print(\"Call 2\")\n",
    "print(log2_with_print(3.0))\n",
    "print(jax.make_jaxpr(log2_with_print)(3.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 1\n",
      "Traced<ShapedArray(float32[1])>with<DynamicJaxprTrace>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.]\n",
      "[3.]\n",
      "Branch 2\n",
      "Traced<ShapedArray(float32[2])>with<DynamicJaxprTrace>\n",
      "[3. 3.]\n",
      "[3. 3.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Retracing\"\"\"\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def ndim_func(x):\n",
    "    print(x)\n",
    "    if x.ndim == 2:\n",
    "        return -x\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"Branch 1\")\n",
    "print(ndim_func(jnp.array([3.0])))\n",
    "print(ndim_func(jnp.array([3.0])))\n",
    "print(\"Branch 2\")\n",
    "print(ndim_func(jnp.array([3.0, 3.0])))\n",
    "print(ndim_func(jnp.array([3.0, 3.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Conditionals\"\"\"\n",
    "\n",
    "\n",
    "# @jax.jit  # Fails\n",
    "def abs_val(x, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i += 1\n",
    "    return x + i\n",
    "\n",
    "\n",
    "print(abs_val(3.0, 5))\n",
    "\n",
    "\n",
    "@jax.jit  # Works (always define outside scope)\n",
    "def loop_body(prev_i):\n",
    "    return prev_i + 1\n",
    "\n",
    "\n",
    "def abs_val_jit(x, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i = loop_body(i)\n",
    "    return x + i\n",
    "\n",
    "\n",
    "print(abs_val_jit(3.0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling\n",
      "8.0\n",
      "9.0\n",
      "Compiling\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Static arguments - needs recompilation for each value of n\"\"\"\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=[\"n\"])  # Or: @jax.jit(static_argnums=(1,))\n",
    "def abs_val_decorated(x, n):\n",
    "    print(\"Compiling\")\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i += 1\n",
    "    return x + i\n",
    "\n",
    "\n",
    "print(abs_val_decorated(3.0, 5))\n",
    "print(abs_val_decorated(4.0, 5))\n",
    "print(abs_val_decorated(4.0, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "10.0\n",
      "6.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Higher-order derivatives\"\"\"\n",
    "\n",
    "f = lambda x: x**3 + 2 * x**2 - 3 * x + 1\n",
    "\n",
    "dfdx = jax.grad(f)\n",
    "d2fdx2 = jax.grad(dfdx)\n",
    "d3fdx3 = jax.grad(d2fdx2)\n",
    "d4fdx4 = jax.grad(d3fdx3)\n",
    "\n",
    "print(dfdx(1.0))\n",
    "print(d2fdx2(1.0))\n",
    "print(d3fdx3(1.0))\n",
    "print(d4fdx4(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_grad: [-0.43314588 -0.7354602  -1.2598921 ]\n",
      "b_grad: -0.690017580986023\n",
      "Both gradients: (Array([-0.43314588, -0.7354602 , -1.2598921 ], dtype=float32), Array(-0.6900176, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Logistic regression example\"\"\"\n",
    "\n",
    "key = jax.random.key(0)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x / 2) + 1)\n",
    "\n",
    "\n",
    "def predict(W, b, inputs):\n",
    "    return sigmoid(jnp.dot(inputs, W) + b)\n",
    "\n",
    "\n",
    "inputs = jnp.array(\n",
    "    [[0.52, 1.12, 0.77], [0.88, -1.08, 0.15], [0.52, 0.06, -1.30], [0.74, -2.49, 1.39]]\n",
    ")\n",
    "\n",
    "targets = jnp.array([True, True, False, True])\n",
    "\n",
    "\n",
    "def loss(W, b):\n",
    "    preds = predict(W, b, inputs)\n",
    "    label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -jnp.sum(jnp.log(label_probs))\n",
    "\n",
    "\n",
    "key, W_key, b_key = jax.random.split(key, 3)\n",
    "W = jax.random.normal(W_key, (3,))\n",
    "b = jax.random.normal(b_key, ())\n",
    "\n",
    "W_grad = jax.grad(loss, argnums=0)(W, b)\n",
    "print(f\"W_grad: {W_grad}\")\n",
    "b_grad = jax.grad(loss, 1)(W, b)\n",
    "print(f\"b_grad: {b_grad}\")\n",
    "grads = jax.grad(loss, argnums=(0, 1))(W, b)\n",
    "print(f\"Both gradients: {grads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': Array([-0.43314588, -0.7354602 , -1.2598921 ], dtype=float32), 'b': Array(-0.6900176, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Differentiating w.r.t. lists, tuples, and dicts\"\"\"\n",
    "\n",
    "\n",
    "def loss2(params_dict):\n",
    "    preds = predict(params_dict[\"W\"], params_dict[\"b\"], inputs)\n",
    "    label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -jnp.sum(jnp.log(label_probs))\n",
    "\n",
    "\n",
    "grads = jax.grad(loss2)({\"W\": W, \"b\": b})\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9729185\n",
      "(Array([-0.43314588, -0.7354602 , -1.2598921 ], dtype=float32), Array(-0.6900176, dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Forward and backwards in one pass\"\"\"\n",
    "\n",
    "loss_val, grads = jax.value_and_grad(loss, (0, 1))(W, b)\n",
    "print(loss_val)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test gradients\"\"\"\n",
    "\n",
    "from jax.test_util import check_grads\n",
    "\n",
    "check_grads(loss, (W, b), order=1)\n",
    "check_grads(loss, (W, b), order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 'a', <object object at 0x7d1ef40c6c80>]   has 3 leaves: [1, 'a', <object object at 0x7d1ef40c6c80>]\n",
      "(1, (2, 3), ())                               has 3 leaves: [1, 2, 3]\n",
      "[1, {'k1': 2, 'k2': (3, 4)}, 5]               has 5 leaves: [1, 2, 3, 4, 5]\n",
      "{'a': 2, 'b': (2, 3)}                         has 3 leaves: [2, 2, 3]\n",
      "Array([1, 2, 3], dtype=int32)                 has 1 leaves: [Array([1, 2, 3], dtype=int32)]\n",
      "[[1, 4, 9], [1, 4], [1, 4, 9, 16]]\n",
      "[[2, 4, 6], [2, 4], [2, 4, 6, 8]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pytrees\"\"\"\n",
    "\n",
    "trees = [\n",
    "    [1, \"a\", object()],\n",
    "    (1, (2, 3), ()),\n",
    "    [1, {\"k1\": 2, \"k2\": (3, 4)}, 5],\n",
    "    {\"a\": 2, \"b\": (2, 3)},\n",
    "    jnp.array([1, 2, 3]),\n",
    "]\n",
    "\n",
    "for pytree in trees:\n",
    "    leaves = jax.tree.leaves(pytree)\n",
    "    print(f\"{repr(pytree):<45} has {len(leaves)} leaves: {leaves}\")\n",
    "\n",
    "list_of_lists = [\n",
    "    [1, 2, 3],\n",
    "    [1, 2],\n",
    "    [1, 2, 3, 4],\n",
    "]\n",
    "\n",
    "print(jax.tree.map(lambda x: x**2, list_of_lists))\n",
    "another_list_of_lists = list_of_lists\n",
    "print(jax.tree.map(lambda x, y: x + y, list_of_lists, another_list_of_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'biases': (4,), 'weights': (1, 4)}, {'biases': (4,), 'weights': (4, 4)}, {'biases': (1,), 'weights': (4, 1)}]\n",
      "[{'weights': array([[-1.1025483 ,  0.68066132, -1.91563952,  0.89516076]]), 'biases': array([1., 1., 1., 1.])}, {'weights': array([[ 0.19618383,  0.00748414, -0.24993495,  0.61554604],\n",
      "       [ 0.02626683,  0.43764833, -0.61300345, -0.36793265],\n",
      "       [-0.62884433, -0.14180592,  0.36823245,  0.71332384],\n",
      "       [ 0.30250433,  1.30756572, -0.98433043,  0.11461864]]), 'biases': array([1., 1., 1., 1.])}, {'weights': array([[ 1.16071927],\n",
      "       [-0.89529667],\n",
      "       [ 1.32671317],\n",
      "       [-0.98126922]]), 'biases': array([1.])}]\n",
      "[{'biases': Array([1.       , 0.9999999, 1.       , 0.9996458], dtype=float32), 'weights': Array([[-1.1025482 ,  0.68066114, -1.9156395 ,  0.8945907 ]], dtype=float32)}, {'biases': Array([1.0004411, 0.9996598, 1.       , 0.9996271], dtype=float32), 'weights': Array([[ 0.19618383,  0.00748414, -0.24993494,  0.61554605],\n",
      "       [ 0.02719115,  0.43693537, -0.61300343, -0.36871406],\n",
      "       [-0.6288443 , -0.14180592,  0.36823246,  0.71332383],\n",
      "       [ 0.30358094,  1.3067353 , -0.9843304 ,  0.11370847]],      dtype=float32)}, {'biases': Array([1.00038], dtype=float32), 'weights': Array([[ 1.1614008 ],\n",
      "       [-0.89335537],\n",
      "       [ 1.3267132 ],\n",
      "       [-0.98107594]], dtype=float32)}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Pytree MLP example\"\"\"\n",
    "\n",
    "\n",
    "def init_mlp_params(layer_widths):\n",
    "    params = []\n",
    "    for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n",
    "        params.append(\n",
    "            dict(\n",
    "                weights=np.random.normal(size=(n_in, n_out)) * np.sqrt(2 / n_in),\n",
    "                biases=np.ones(shape=(n_out,)),\n",
    "            )\n",
    "        )\n",
    "    return params\n",
    "\n",
    "\n",
    "params = init_mlp_params([1, 4, 4, 1])\n",
    "print(jax.tree.map(lambda x: x.shape, params))\n",
    "\n",
    "\n",
    "def forward(params, x):\n",
    "    *hidden, last = params\n",
    "    for layer in hidden:\n",
    "        x = jax.nn.relu(x @ layer[\"weights\"] + layer[\"biases\"])\n",
    "    return x @ last[\"weights\"] + last[\"biases\"]\n",
    "\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    return jnp.mean((forward(params, x) - y) ** 2)\n",
    "\n",
    "\n",
    "eta = 1e-4\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update(params, x, y):\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "    return jax.tree.map(lambda p, g: p - eta * g, params, grads)\n",
    "\n",
    "\n",
    "print(params)\n",
    "print(update(params, np.array([[1.0], [2.0]]), np.array([[0.0], [0.0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f32[5,4] from the argument W1\n",
      "f32[6,5] from the argument W2\n",
      "f32[7,6] from the argument W3\n",
      "f32[4] from the argument x\n",
      "f32[5] output of sin from /tmp/ipykernel_1161818/916765993.py:5:11 (g)\n",
      "f32[5] output of cos from /tmp/ipykernel_1161818/916765993.py:5:11 (g)\n",
      "f32[6] output of sin from /tmp/ipykernel_1161818/916765993.py:5:11 (g)\n",
      "f32[6] output of cos from /tmp/ipykernel_1161818/916765993.py:5:11 (g)\n",
      "f32[7] output of cos from /tmp/ipykernel_1161818/916765993.py:5:11 (g)\n",
      "\n",
      "f32[5,4] from the argument W1\n",
      "f32[6,5] from the argument W2\n",
      "f32[7,6] from the argument W3\n",
      "f32[4] from the argument x\n",
      "f32[5] output of sin from /tmp/ipykernel_1161818/916765993.py:5:11 (g)\n",
      "f32[6] output of sin from /tmp/ipykernel_1161818/916765993.py:5:11 (g)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Gradient checkpointing\"\"\"\n",
    "\n",
    "\n",
    "def g(W, x):\n",
    "    y = jnp.dot(W, x)\n",
    "    return jnp.sin(y)\n",
    "\n",
    "\n",
    "def f(W1, W2, W3, x):\n",
    "    x = g(W1, x)\n",
    "    x = g(W2, x)\n",
    "    x = g(W3, x)\n",
    "    return x\n",
    "\n",
    "\n",
    "W1 = jnp.ones((5, 4))\n",
    "W2 = jnp.ones((6, 5))\n",
    "W3 = jnp.ones((7, 6))\n",
    "x = jnp.ones(4)\n",
    "\n",
    "# Inspect residuals\n",
    "jax.ad_checkpoint.print_saved_residuals(f, W1, W2, W3, x)\n",
    "print()\n",
    "\n",
    "\n",
    "# Force not to save residuals\n",
    "def f2(W1, W2, W3, x):\n",
    "    x = jax.checkpoint(g)(W1, x)\n",
    "    x = jax.checkpoint(g)(W2, x)\n",
    "    x = jax.checkpoint(g)(W3, x)\n",
    "    return x\n",
    "\n",
    "\n",
    "jax.ad_checkpoint.print_saved_residuals(f2, W1, W2, W3, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
